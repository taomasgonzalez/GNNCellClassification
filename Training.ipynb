{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eacdd1a",
   "metadata": {},
   "source": [
    "# Training GCN\n",
    "Train a Graph Neural Network with the histology + gene information.\n",
    "\n",
    "We are going to create a brain layer classifier using [Torch Geometric's GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GCN.html?highlight=gcn#torch-geometric-nn-models-gcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0c564",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "For a detailed exploration and analysis of what the data actually contains, visit the `DataAnalysis` notebook, located in the same directory as this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ffdf9",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79ac9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/projects/GNNCellClassification/dataset ~/projects/GNNCellClassification\n",
      "Don't download data: Both data and images exists\n",
      "~/projects/GNNCellClassification\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "./dataset/getdata.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4171a",
   "metadata": {},
   "source": [
    "### Actual loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8108b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913c178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData for sample 151676 …\n",
      "Loading AnnData for sample 151669 …\n",
      "Loading AnnData for sample 151507 …\n",
      "Loading AnnData for sample 151508 …\n",
      "Loading AnnData for sample 151672 …\n",
      "Loading AnnData for sample 151670 …\n",
      "Loading AnnData for sample 151673 …\n",
      "Loading AnnData for sample 151675 …\n",
      "Loading AnnData for sample 151510 …\n",
      "Loading AnnData for sample 151671 …\n",
      "Loading AnnData for sample 151674 …\n",
      "Loading AnnData for sample 151509 …\n",
      "Loading Image for sample 151676 …\n",
      "Loading Image for sample 151669 …\n",
      "Loading Image for sample 151507 …\n",
      "Loading Image for sample 151508 …\n",
      "Loading Image for sample 151672 …\n",
      "Loading Image for sample 151670 …\n",
      "Loading Image for sample 151673 …\n",
      "Loading Image for sample 151675 …\n",
      "Loading Image for sample 151510 …\n",
      "Loading Image for sample 151671 …\n",
      "Loading Image for sample 151674 …\n",
      "Loading Image for sample 151509 …\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "import preprocess\n",
    "\n",
    "data_dir, img_dir, graph_dir = \"dataset/data\", \"dataset/images\", \"out/graphs\"\n",
    "ann_data, histology_imgs = preprocess.main(data_dir, img_dir, graph_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e691cc",
   "metadata": {},
   "source": [
    "## Features used for training\n",
    "\n",
    "We are going to use the following features for training:\n",
    "\n",
    "**Edge Features**:\n",
    "- Spatial Connectivities` between spots\n",
    "- Pixel `distance` (adjusted by color)\n",
    "\n",
    "**Node Features**:\n",
    "- `UMI` count (log)\n",
    "- `Color` in the `neighbourhood` of the spot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04272538",
   "metadata": {},
   "source": [
    "## PyTorch Geometric's data structure\n",
    "\n",
    "From the official [Documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GCN.html?highlight=gcn#torch-geometric-nn-models-gcn) , we can see that:\n",
    "\n",
    "A single graph in PyG is described by an instance of `torch_geometric.data.Data`, which holds the following attributes by default:\n",
    "\n",
    "- `data.x`: Node feature matrix with shape `[num_nodes, num_node_features]`\n",
    "\n",
    "- `data.edge_index`: Graph connectivity in `COO` format with shape `[2, num_edges]` and type `torch.long`\n",
    "\n",
    "- `data.edge_attr`: Edge feature matrix with shape `[num_edges, num_edge_features]`\n",
    "\n",
    "- `data.y`: Target to train against (may have arbitrary shape), e.g., node-level targets of shape `[num_nodes, *]` or graph-level targets of shape `[1, *]`\n",
    "\n",
    "- `data.pos`: Node position matrix with shape `[num_nodes, num_dimensions]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e296d84",
   "metadata": {},
   "source": [
    "## Creating the required data structures for training\n",
    "\n",
    "We need to convert the data to what's required by PyTorch geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83907ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2460ade",
   "metadata": {},
   "source": [
    "We randomly select which patients will be used for training, which ones for validation and which ones for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7fe14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_patients: ['151674', '151669', '151676', '151672', '151508', '151509', '151673', '151671']\n",
      "val_patients: ['151507', '151675']\n",
      "test_patients: ['151670', '151510']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "train_patients = random.sample(list(ann_data.keys()), 8)\n",
    "print(f\"train_patients: {train_patients}\")\n",
    "\n",
    "rest = [patient for patient in ann_data.keys() if patient not in train_patients]\n",
    "val_patients = random.sample(rest, 2)\n",
    "print(f\"val_patients: {val_patients}\")\n",
    "test_patients = [patient for patient in rest if patient not in val_patients]\n",
    "print(f\"test_patients: {test_patients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb65883",
   "metadata": {},
   "source": [
    "### data.edge_index\n",
    "We need to transform to a `PyTorch` tensor in `COO` format.\n",
    "Let's start with a reference patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a55bbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 20052 stored elements and shape (3460, 3460)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_data['151676'].obsp['spatial_connectivities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2093cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ann_data['151676'].obsp['spatial_connectivities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ef8510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<COOrdinate sparse matrix of dtype 'float64'\n",
       "\twith 20052 stored elements and shape (3460, 3460)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_matrix = ann_data['151676'].obsp['spatial_connectivities'].tocoo()\n",
    "coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d336332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._coo.coo_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(coo_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63168e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 151676: (3460, 3460)\n",
      "Patient 151669: (3661, 3661)\n",
      "Patient 151507: (4226, 4226)\n",
      "Patient 151508: (4384, 4384)\n",
      "Patient 151672: (4015, 4015)\n",
      "Patient 151670: (3498, 3498)\n",
      "Patient 151673: (3639, 3639)\n",
      "Patient 151675: (3592, 3592)\n",
      "Patient 151510: (4634, 4634)\n",
      "Patient 151671: (4110, 4110)\n",
      "Patient 151674: (3673, 3673)\n",
      "Patient 151509: (4789, 4789)\n"
     ]
    }
   ],
   "source": [
    "coo_connections = { patient: data.obsp['spatial_connectivities'].tocoo()  \\\n",
    "                   for patient, data in ann_data.items() }\n",
    "for patient, coo in coo_connections.items():\n",
    "    print(f\"Patient {patient}: {coo.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0178ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151676: torch.Size([2, 20052])\n",
      "151669: torch.Size([2, 21194])\n",
      "151507: torch.Size([2, 24770])\n",
      "151508: torch.Size([2, 25698])\n",
      "151672: torch.Size([2, 23382])\n",
      "151670: torch.Size([2, 20370])\n",
      "151673: torch.Size([2, 21124])\n",
      "151675: torch.Size([2, 20762])\n",
      "151510: torch.Size([2, 27198])\n",
      "151671: torch.Size([2, 24052])\n",
      "151674: torch.Size([2, 21258])\n",
      "151509: torch.Size([2, 28172])\n"
     ]
    }
   ],
   "source": [
    "edge_indices = {}\n",
    "\n",
    "for patient, coo in coo_connections.items():\n",
    "    row = torch.from_numpy(coo.row).long()\n",
    "    col = torch.from_numpy(coo.col).long()\n",
    "    edge_indices[patient] = torch.stack([row, col], dim=0)\n",
    "\n",
    "    print(f\"{patient}: {edge_indices[patient].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0682b",
   "metadata": {},
   "source": [
    "### data.edge_attr\n",
    "Edge feature matrix with shape `[num_edges, num_edge_features]`.\n",
    "For now, only get the distances for the ones that are spatially connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e38247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151676: torch.Size([20052, 1])\n",
      "151669: torch.Size([21194, 1])\n",
      "151507: torch.Size([24770, 1])\n",
      "151508: torch.Size([25698, 1])\n",
      "151672: torch.Size([23382, 1])\n",
      "151670: torch.Size([20370, 1])\n",
      "151673: torch.Size([21124, 1])\n",
      "151675: torch.Size([20762, 1])\n",
      "151510: torch.Size([27198, 1])\n",
      "151671: torch.Size([24052, 1])\n",
      "151674: torch.Size([21258, 1])\n",
      "151509: torch.Size([28172, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "edge_features = {}\n",
    "for patient in ann_data.keys():\n",
    "    filename = str(f\"{patient}_adj.npy\")\n",
    "    adj_distances = np.load(os.path.join(graph_dir, filename))\n",
    "    adj_tensor = torch.from_numpy(adj_distances)\n",
    "    row, col = edge_indices[patient]\n",
    "    distances = adj_tensor[row, col]\n",
    "    edge_features[patient] = distances.unsqueeze(1).float()\n",
    "    print(f\"{patient}: {edge_features[patient].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344688e",
   "metadata": {},
   "source": [
    "### data.x\n",
    "Node feature matrix with shape `[num_nodes, num_node_features]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498c508",
   "metadata": {},
   "source": [
    "#### Normalizing UMI count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27af5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "normalized_data = {}\n",
    "for patient in ann_data.keys():\n",
    "    sc.pp.normalize_total(ann_data[patient])\n",
    "    sc.pp.log1p(ann_data[patient])\n",
    "    normalized_data[patient] = ann_data[patient].X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4dead",
   "metadata": {},
   "source": [
    "#### Reducing Dimensionality of data.x\n",
    "Apply `PCA` Principal Component Analysis on the gene expression count to reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "589601f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3460, 50)\n",
      "(3661, 50)\n",
      "(4226, 50)\n",
      "(4384, 50)\n",
      "(4015, 50)\n",
      "(3498, 50)\n",
      "(3639, 50)\n",
      "(3592, 50)\n",
      "(4634, 50)\n",
      "(4110, 50)\n",
      "(3673, 50)\n",
      "(4789, 50)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import vstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "train_data = list(data for patient, data in normalized_data.items() if patient in train_patients)\n",
    "assert len(train_data) == 8\n",
    "train_stack = vstack(list(train_data))\n",
    "# svd.components_ now holds a shared basis between all the spots in the training set.\n",
    "# This basis will be freezed for using it when validating and testing.\n",
    "svd = TruncatedSVD(n_components=50, random_state=0)\n",
    "svd.fit(train_stack)\n",
    "\n",
    "reduced_data = {patient: svd.transform(X) for patient, X in normalized_data.items()}\n",
    "for data in reduced_data.values():\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df58bd",
   "metadata": {},
   "source": [
    "#### Retrieving histology color information for data.x\n",
    "Add it to the data.x matrix as an extra feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3274be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variances of color0, color1, color2 =  [0.00134047 0.00493318 0.00063244]\n",
      "(3460,)\n",
      "Variances of color0, color1, color2 =  [0.00100299 0.00408081 0.00090281]\n",
      "(3661,)\n",
      "Variances of color0, color1, color2 =  [0.00058533 0.00256631 0.00049386]\n",
      "(4226,)\n",
      "Variances of color0, color1, color2 =  [0.00059817 0.00289116 0.00048598]\n",
      "(4384,)\n",
      "Variances of color0, color1, color2 =  [0.00160404 0.00482472 0.00077629]\n",
      "(4015,)\n",
      "Variances of color0, color1, color2 =  [0.00137143 0.00389194 0.00066271]\n",
      "(3498,)\n",
      "Variances of color0, color1, color2 =  [0.00084196 0.00644021 0.00071149]\n",
      "(3639,)\n",
      "Variances of color0, color1, color2 =  [0.00166604 0.00636242 0.00100769]\n",
      "(3592,)\n",
      "Variances of color0, color1, color2 =  [0.00063251 0.00230641 0.00054535]\n",
      "(4634,)\n",
      "Variances of color0, color1, color2 =  [0.00228128 0.00711623 0.00111823]\n",
      "(4110,)\n",
      "Variances of color0, color1, color2 =  [0.00123189 0.00723225 0.00067618]\n",
      "(3673,)\n",
      "Variances of color0, color1, color2 =  [0.00053539 0.00314728 0.00054947]\n",
      "(4789,)\n"
     ]
    }
   ],
   "source": [
    "from graph import get_region_colors\n",
    "\n",
    "offsets = {'151676': 310, '151669': 276, '151507': 236, '151508': 232, '151672': 264, \\\n",
    "        '151670': 339, '151673': 260, '151675': 228, '151510': 204, '151671': 238, \\\n",
    "        '151674': 234, '151509': 220}\n",
    "thickness = 48\n",
    "\n",
    "normalized_color_avgs = {}\n",
    "for patient_id, data in ann_data.items():\n",
    "    offset = offsets[patient_id]\n",
    "    hires_scale = ann_data[patient_id].uns['spatial'][patient_id]['scalefactors']['tissue_hires_scalef']\n",
    "    spot_pixels = ann_data[patient_id].obsm['spatial'] * hires_scale\n",
    "    spot_pixels = spot_pixels.astype(int)\n",
    "    \n",
    "    image = ann_data[patient_id].uns['spatial'][patient_id]['images']['hires']\n",
    "    hires_shape = image.shape\n",
    "    assert min(hires_shape[0], hires_shape[1]) > spot_pixels.max()\n",
    "\n",
    "    flipped_image = np.flip(image, 0)\n",
    "    x_pixels = spot_pixels[:, 0]\n",
    "    y_pixels = spot_pixels[:, 1]\n",
    "    \n",
    "    normalized_color_avgs[patient_id] = get_region_colors(x_pixels, y_pixels, offset=offsets[patient_id], image=flipped_image, thickness=thickness, alpha=1)\n",
    "    print(normalized_color_avgs[patient_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27933ffa",
   "metadata": {},
   "source": [
    "#### Integrating UMI count + color information\n",
    "Integrate the normalized and dimensionality-reduced `UMI` count information with the `color` information to form the data.x matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3277de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3460, 51])\n",
      "torch.Size([3661, 51])\n",
      "torch.Size([4226, 51])\n",
      "torch.Size([4384, 51])\n",
      "torch.Size([4015, 51])\n",
      "torch.Size([3498, 51])\n",
      "torch.Size([3639, 51])\n",
      "torch.Size([3592, 51])\n",
      "torch.Size([4634, 51])\n",
      "torch.Size([4110, 51])\n",
      "torch.Size([3673, 51])\n",
      "torch.Size([4789, 51])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data_x = {}\n",
    "for patient_id, data in ann_data.items():\n",
    "    pca = reduced_data[patient_id]\n",
    "    color_avgs = normalized_color_avgs[patient_id]\n",
    "    color_avgs = color_avgs.reshape(-1, 1)\n",
    "    x_np = np.hstack([pca, color_avgs])\n",
    "    \n",
    "    x_tensor = torch.from_numpy(x_np).float()\n",
    "    \n",
    "    data_x[patient_id] = x_tensor\n",
    "    print(data_x[patient_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b7dc2",
   "metadata": {},
   "source": [
    "### data.y\n",
    "Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "In our case, the target is the brian layer for each node, so it's going to be of shape [num_nodes, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a91229",
   "metadata": {},
   "source": [
    "As each brain layer category is a string, we should use their values to get a tensor out of them instead. This can be easily done in AnnData:\n",
    "\n",
    "- `Layer 1` -> `0`\n",
    "- `Layer 2` -> `1`\n",
    "- `Layer 3` -> `2`\n",
    "- `Layer 4` -> `3`\n",
    "- `Layer 5` -> `4`\n",
    "- `Layer 6` -> `5`\n",
    "- `Layer WM` -> `6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6919fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer categories: AAACAAGTATCTCCCA-1    Layer3\n",
      "AAACAATCTACTAGCA-1    Layer1\n",
      "AAACACCAATAACTGC-1        WM\n",
      "AAACAGAGCGACTCCT-1    Layer3\n",
      "Name: sce.layer_guess, dtype: category\n",
      "Categories (7, object): ['Layer1', 'Layer2', 'Layer3', 'Layer4', 'Layer5', 'Layer6', 'WM']\n",
      "--------\n",
      "layer codes: AAACAAGTATCTCCCA-1    2\n",
      "AAACAATCTACTAGCA-1    0\n",
      "AAACACCAATAACTGC-1    6\n",
      "AAACAGAGCGACTCCT-1    2\n",
      "dtype: int8\n"
     ]
    }
   ],
   "source": [
    "print(\"layer categories:\" , ann_data['151676'].obs[\"sce.layer_guess\"].head(4))\n",
    "print(\"--------\")\n",
    "print(\"layer codes:\" , ann_data['151676'].obs[\"sce.layer_guess\"].cat.codes.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af21b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 6,  ..., 6, 5, 0], dtype=torch.int8)\n",
      "tensor([1, 3, 0,  ..., 3, 2, 0], dtype=torch.int8)\n",
      "tensor([0, 2, 0,  ..., 6, 5, 0], dtype=torch.int8)\n",
      "tensor([2, 0, 6,  ..., 4, 6, 0], dtype=torch.int8)\n",
      "tensor([2, 3, 0,  ..., 3, 3, 0], dtype=torch.int8)\n",
      "tensor([1, 3, 0,  ..., 3, 2, 0], dtype=torch.int8)\n",
      "tensor([2, 0, 6,  ..., 5, 6, 1], dtype=torch.int8)\n",
      "tensor([0, 2, 6,  ..., 6, 5, 6], dtype=torch.int8)\n",
      "tensor([0, 4, 2,  ..., 2, 5, 2], dtype=torch.int8)\n",
      "tensor([2, 3, 0,  ..., 2, 4, 0], dtype=torch.int8)\n",
      "tensor([2, 0, 6,  ..., 5, 6, 0], dtype=torch.int8)\n",
      "tensor([0, 2, 5,  ..., 3, 5, 1], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "data_y = {}\n",
    "for patient_id, data in ann_data.items():\n",
    "    data_y[patient_id] = torch.tensor(data.obs[\"sce.layer_guess\"].cat.codes.values, dtype=torch.int8)\n",
    "    print(data_y[patient_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544b7fe",
   "metadata": {},
   "source": [
    "## data.pos\n",
    "Node position matrix with shape [num_nodes, num_dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bdce801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3460, 2])\n",
      "torch.Size([3661, 2])\n",
      "torch.Size([4226, 2])\n",
      "torch.Size([4384, 2])\n",
      "torch.Size([4015, 2])\n",
      "torch.Size([3498, 2])\n",
      "torch.Size([3639, 2])\n",
      "torch.Size([3592, 2])\n",
      "torch.Size([4634, 2])\n",
      "torch.Size([4110, 2])\n",
      "torch.Size([3673, 2])\n",
      "torch.Size([4789, 2])\n"
     ]
    }
   ],
   "source": [
    "data_pos = {}\n",
    "\n",
    "for patient_id, data in ann_data.items():\n",
    "    y_tensor = torch.from_numpy(data.obs[['array_row', 'array_col']].values)\n",
    "    y_tensor = y_tensor.to(torch.int16)\n",
    "    data_pos[patient_id] = y_tensor\n",
    "    print(data_pos[patient_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95772931",
   "metadata": {},
   "source": [
    "## Creating the Data Loader with all the gathered information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf0b6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "datalist = []\n",
    "for patient_id in ann_data.keys():\n",
    "    datalist.append(Data(\n",
    "        x=data_x[patient_id],\n",
    "        edge_index=edge_indices[patient_id],\n",
    "        edge_attr=edge_features[patient_id],\n",
    "        pos=data_pos[patient_id],\n",
    "        y=data_y[patient_id]\n",
    "    ))\n",
    "\n",
    "loader = DataLoader(dataset=datalist, batch_size=16, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (general-venv)",
   "language": "python",
   "name": "general-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
