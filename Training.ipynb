{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eacdd1a",
   "metadata": {},
   "source": [
    "# Training GCN\n",
    "\n",
    "Train a Graph Neural Network with the histology + gene information.\n",
    "\n",
    "We are going to create a brain layer classifier using [Torch Geometric's GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GCN.html?highlight=gcn#torch-geometric-nn-models-gcn)\n",
    "\n",
    "This notebook shows the processing steps (at a lower level, what goes below the wrappers) taken from acquiring the data to creating the dataloader and using those in a training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0c564",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "For a detailed exploration and analysis of what the data actually contains, visit the `DataAnalysis` notebook, located in the same directory as this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ffdf9",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79ac9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/projects/GNNCellClassification/dataset ~/projects/GNNCellClassification\n",
      "Downloading data if needed...\n",
      "Don't download data: Both data and images already exist\n",
      "~/projects/GNNCellClassification\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "./dataset/getdata.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4171a",
   "metadata": {},
   "source": [
    "### Actual loading + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8108b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913c178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData for sample 151676 …\n",
      "Loading AnnData for sample 151669 …\n",
      "Loading AnnData for sample 151507 …\n",
      "Loading AnnData for sample 151508 …\n",
      "Loading AnnData for sample 151672 …\n",
      "Loading AnnData for sample 151670 …\n",
      "Loading AnnData for sample 151673 …\n",
      "Loading AnnData for sample 151675 …\n",
      "Loading AnnData for sample 151510 …\n",
      "Loading AnnData for sample 151671 …\n",
      "Loading AnnData for sample 151674 …\n",
      "Loading AnnData for sample 151509 …\n",
      "Loading Image for sample 151676 …\n",
      "Loading Image for sample 151669 …\n",
      "Loading Image for sample 151507 …\n",
      "Loading Image for sample 151508 …\n",
      "Loading Image for sample 151672 …\n",
      "Loading Image for sample 151670 …\n",
      "Loading Image for sample 151673 …\n",
      "Loading Image for sample 151675 …\n",
      "Loading Image for sample 151510 …\n",
      "Loading Image for sample 151671 …\n",
      "Loading Image for sample 151674 …\n",
      "Loading Image for sample 151509 …\n",
      "Creating Graphs for sample 151676 …\n",
      "Calculating adj matrix using histology image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@8.026] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variances of color0, color1, color2 =  [0.00134047 0.00493318 0.00063244]\n",
      "Var of x, y, z =  99580.40478290287 115934.31450257276 115934.31589490475\n",
      "Max value:  1942.6682021906095\n",
      "Creating Graphs for sample 151669 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00100299 0.00408081 0.00090281]\n",
      "Var of x, y, z =  115055.61305404994 109785.12825823567 115055.62076952032\n",
      "Max value:  1990.445338189566\n",
      "Creating Graphs for sample 151507 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00058533 0.00256631 0.00049386]\n",
      "Var of x, y, z =  119744.2901290638 140702.3686761846 140702.35921238275\n",
      "Max value:  2613.207049095901\n",
      "Creating Graphs for sample 151508 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00059817 0.00289116 0.00048598]\n",
      "Var of x, y, z =  121008.39013112546 148803.96100264232 148803.96221282193\n",
      "Max value:  2685.33787054901\n",
      "Creating Graphs for sample 151672 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00160404 0.00482472 0.00077629]\n",
      "Var of x, y, z =  125955.08941804472 120889.49315819104 125955.0822136935\n",
      "Max value:  2040.7540693425203\n",
      "Creating Graphs for sample 151670 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00137143 0.00389194 0.00066271]\n",
      "Var of x, y, z =  117421.16966715605 96262.35166325544 117421.17944705991\n",
      "Max value:  1994.7956641598917\n",
      "Creating Graphs for sample 151673 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00084196 0.00644021 0.00071149]\n",
      "Var of x, y, z =  100566.62435006673 126180.78424892435 126180.78032585149\n",
      "Max value:  2233.4195393409327\n",
      "Creating Graphs for sample 151675 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00166604 0.00636242 0.00100769]\n",
      "Var of x, y, z =  109225.92385384682 122473.29652010654 122473.30224166339\n",
      "Max value:  2084.929972195824\n",
      "Creating Graphs for sample 151510 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00063251 0.00230641 0.00054535]\n",
      "Var of x, y, z =  138072.9741057493 146542.48700947323 146542.4728899039\n",
      "Max value:  2798.643669443764\n",
      "Creating Graphs for sample 151671 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00228128 0.00711623 0.00111823]\n",
      "Var of x, y, z =  128154.57227603435 124286.5513145198 128154.56169854588\n",
      "Max value:  1865.3166589559444\n",
      "Creating Graphs for sample 151674 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00123189 0.00723225 0.00067618]\n",
      "Var of x, y, z =  98249.27241526509 131857.30198876595 131857.3024035793\n",
      "Max value:  2136.399617973797\n",
      "Creating Graphs for sample 151509 …\n",
      "Calculating adj matrix using histology image...\n",
      "Variances of color0, color1, color2 =  [0.00053539 0.00314728 0.00054947]\n",
      "Var of x, y, z =  139970.52544816612 156512.8461309482 156512.83344561214\n",
      "Max value:  2894.1237944718246\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "import preprocess\n",
    "import load_data\n",
    "\n",
    "data_dir, img_dir, graph_dir = \"dataset/data\", \"dataset/images\", \"out/graphs\"\n",
    "ann_data, histology_imgs = preprocess.main(data_dir, img_dir, graph_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e691cc",
   "metadata": {},
   "source": [
    "## Features used for training\n",
    "\n",
    "We are going to use the following features for training:\n",
    "\n",
    "**Edge Features**:\n",
    "- Spatial Connectivities` between spots\n",
    "- Pixel `distance` (adjusted by color)\n",
    "\n",
    "**Node Features**:\n",
    "- `UMI` count (log)\n",
    "- `Color` in the `neighbourhood` of the spot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04272538",
   "metadata": {},
   "source": [
    "## PyTorch Geometric's data structure\n",
    "\n",
    "From the official [Documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GCN.html?highlight=gcn#torch-geometric-nn-models-gcn) , we can see that:\n",
    "\n",
    "A single graph in PyG is described by an instance of `torch_geometric.data.Data`, which holds the following attributes by default:\n",
    "\n",
    "- `data.x`: Node feature matrix with shape `[num_nodes, num_node_features]`\n",
    "\n",
    "- `data.edge_index`: Graph connectivity in `COO` format with shape `[2, num_edges]` and type `torch.long`\n",
    "\n",
    "- `data.edge_attr`: Edge feature matrix with shape `[num_edges, num_edge_features]`\n",
    "\n",
    "- `data.y`: Target to train against (may have arbitrary shape), e.g., node-level targets of shape `[num_nodes, *]` or graph-level targets of shape `[1, *]`\n",
    "\n",
    "- `data.pos`: Node position matrix with shape `[num_nodes, num_dimensions]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e296d84",
   "metadata": {},
   "source": [
    "## Creating the required data structures for training\n",
    "\n",
    "We need to convert the data to what's required by PyTorch geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83907ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2460ade",
   "metadata": {},
   "source": [
    "We randomly select which patients will be used for training, which ones for validation and which ones for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7fe14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_patients: ['151507', '151675']\n",
      "train_patients: ['151674', '151669', '151676', '151672', '151508', '151509', '151673', '151671']\n",
      "val_patients: ['151507', '151675']\n",
      "test_patients: ['151670', '151510']\n"
     ]
    }
   ],
   "source": [
    "from dataloader import train_val_test_split\n",
    "\n",
    "\n",
    "train_patients, val_patients, test_patients = train_val_test_split(ann_data=ann_data, seed=42)\n",
    "\n",
    "print(f\"train_patients: {train_patients}\")\n",
    "print(f\"val_patients: {val_patients}\")\n",
    "print(f\"test_patients: {test_patients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb65883",
   "metadata": {},
   "source": [
    "### data.edge_index\n",
    "We need to transform to a `PyTorch` tensor in `COO` format.\n",
    "Let's start with a reference patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a55bbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 20052 stored elements and shape (3460, 3460)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_data['151676'].obsp['spatial_connectivities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2093cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ann_data['151676'].obsp['spatial_connectivities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ef8510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<COOrdinate sparse matrix of dtype 'float64'\n",
       "\twith 20052 stored elements and shape (3460, 3460)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_matrix = ann_data['151676'].obsp['spatial_connectivities'].tocoo()\n",
    "coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d336332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._coo.coo_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(coo_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63168e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 151676: (3460, 3460)\n",
      "Patient 151669: (3661, 3661)\n",
      "Patient 151507: (4226, 4226)\n",
      "Patient 151508: (4384, 4384)\n",
      "Patient 151672: (4015, 4015)\n",
      "Patient 151670: (3498, 3498)\n",
      "Patient 151673: (3639, 3639)\n",
      "Patient 151675: (3592, 3592)\n",
      "Patient 151510: (4634, 4634)\n",
      "Patient 151671: (4110, 4110)\n",
      "Patient 151674: (3673, 3673)\n",
      "Patient 151509: (4789, 4789)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_coo_connections\n",
    "\n",
    "coo_connections = get_coo_connections(ann_data)\n",
    "for patient, coo in coo_connections.items():\n",
    "    print(f\"Patient {patient}: {coo.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0178ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151676: torch.Size([2, 20052])\n",
      "151669: torch.Size([2, 21194])\n",
      "151507: torch.Size([2, 24770])\n",
      "151508: torch.Size([2, 25698])\n",
      "151672: torch.Size([2, 23382])\n",
      "151670: torch.Size([2, 20370])\n",
      "151673: torch.Size([2, 21124])\n",
      "151675: torch.Size([2, 20762])\n",
      "151510: torch.Size([2, 27198])\n",
      "151671: torch.Size([2, 24052])\n",
      "151674: torch.Size([2, 21258])\n",
      "151509: torch.Size([2, 28172])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_edge_indices\n",
    "\n",
    "edge_indices = get_edge_indices(coo_connections)\n",
    "\n",
    "for patient, index in edge_indices.items():\n",
    "    print(f\"{patient}: {edge_indices[patient].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0682b",
   "metadata": {},
   "source": [
    "### data.edge_attr\n",
    "Edge feature matrix with shape `[num_edges, num_edge_features]`.\n",
    "For now, only get the distances for the ones that are spatially connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e38247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151676: torch.Size([20052, 1])\n",
      "151669: torch.Size([21194, 1])\n",
      "151507: torch.Size([24770, 1])\n",
      "151508: torch.Size([25698, 1])\n",
      "151672: torch.Size([23382, 1])\n",
      "151670: torch.Size([20370, 1])\n",
      "151673: torch.Size([21124, 1])\n",
      "151675: torch.Size([20762, 1])\n",
      "151510: torch.Size([27198, 1])\n",
      "151671: torch.Size([24052, 1])\n",
      "151674: torch.Size([21258, 1])\n",
      "151509: torch.Size([28172, 1])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_edge_features\n",
    "\n",
    "edge_features = get_edge_features(ann_data, edge_indices, graph_dir)\n",
    "for patient in ann_data.keys():\n",
    "    print(f\"{patient}: {edge_features[patient].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344688e",
   "metadata": {},
   "source": [
    "### data.x\n",
    "Node feature matrix with shape `[num_nodes, num_node_features]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498c508",
   "metadata": {},
   "source": [
    "#### Normalizing UMI count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27af5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_normalized_umi_count\n",
    "\n",
    "normalized_data = get_normalized_umi_count(ann_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4dead",
   "metadata": {},
   "source": [
    "#### Reducing Dimensionality of data.x\n",
    "Apply `PCA` Principal Component Analysis on the gene expression count to reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "589601f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3460, 55)\n",
      "(3661, 55)\n",
      "(4226, 55)\n",
      "(4384, 55)\n",
      "(4015, 55)\n",
      "(3498, 55)\n",
      "(3639, 55)\n",
      "(3592, 55)\n",
      "(4634, 55)\n",
      "(4110, 55)\n",
      "(3673, 55)\n",
      "(4789, 55)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_pca_reduced\n",
    "\n",
    "reduced_data = get_pca_reduced(normalized_data, train_patients, n_components=55)\n",
    "for data in reduced_data.values():\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df58bd",
   "metadata": {},
   "source": [
    "#### Retrieving histology color information for data.x\n",
    "Add it to the data.x matrix as an extra feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3274be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variances of color0, color1, color2 =  [0.00134047 0.00493318 0.00063244]\n",
      "Variances of color0, color1, color2 =  [0.00100299 0.00408081 0.00090281]\n",
      "Variances of color0, color1, color2 =  [0.00058533 0.00256631 0.00049386]\n",
      "Variances of color0, color1, color2 =  [0.00059817 0.00289116 0.00048598]\n",
      "Variances of color0, color1, color2 =  [0.00160404 0.00482472 0.00077629]\n",
      "Variances of color0, color1, color2 =  [0.00137143 0.00389194 0.00066271]\n",
      "Variances of color0, color1, color2 =  [0.00084196 0.00644021 0.00071149]\n",
      "Variances of color0, color1, color2 =  [0.00166604 0.00636242 0.00100769]\n",
      "Variances of color0, color1, color2 =  [0.00063251 0.00230641 0.00054535]\n",
      "Variances of color0, color1, color2 =  [0.00228128 0.00711623 0.00111823]\n",
      "Variances of color0, color1, color2 =  [0.00123189 0.00723225 0.00067618]\n",
      "Variances of color0, color1, color2 =  [0.00053539 0.00314728 0.00054947]\n",
      "(3460,)\n",
      "(3661,)\n",
      "(4226,)\n",
      "(4384,)\n",
      "(4015,)\n",
      "(3498,)\n",
      "(3639,)\n",
      "(3592,)\n",
      "(4634,)\n",
      "(4110,)\n",
      "(3673,)\n",
      "(4789,)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_normalized_color_avgs\n",
    "\n",
    "normalized_color_avgs = get_normalized_color_avgs(ann_data)\n",
    "\n",
    "for patient_id in normalized_color_avgs.keys():\n",
    "    print(normalized_color_avgs[patient_id].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27933ffa",
   "metadata": {},
   "source": [
    "#### Integrating UMI count + color information\n",
    "Integrate the normalized and dimensionality-reduced `UMI` count information with the `color` information to form the data.x matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3277de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3460, 56])\n",
      "torch.Size([3661, 56])\n",
      "torch.Size([4226, 56])\n",
      "torch.Size([4384, 56])\n",
      "torch.Size([4015, 56])\n",
      "torch.Size([3498, 56])\n",
      "torch.Size([3639, 56])\n",
      "torch.Size([3592, 56])\n",
      "torch.Size([4634, 56])\n",
      "torch.Size([4110, 56])\n",
      "torch.Size([3673, 56])\n",
      "torch.Size([4789, 56])\n",
      "torch.Size([3460, 56])\n",
      "torch.Size([3661, 56])\n",
      "torch.Size([4226, 56])\n",
      "torch.Size([4384, 56])\n",
      "torch.Size([4015, 56])\n",
      "torch.Size([3498, 56])\n",
      "torch.Size([3639, 56])\n",
      "torch.Size([3592, 56])\n",
      "torch.Size([4634, 56])\n",
      "torch.Size([4110, 56])\n",
      "torch.Size([3673, 56])\n",
      "torch.Size([4789, 56])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_data_x\n",
    "\n",
    "data_x = get_data_x(ann_data, reduced_data, normalized_color_avgs)\n",
    "for patient_id in data_x.keys():\n",
    "    print(data_x[patient_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b7dc2",
   "metadata": {},
   "source": [
    "### data.y\n",
    "Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "\n",
    "In our case, the target is the brian layer for each node, so it's going to be of shape [num_nodes, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a91229",
   "metadata": {},
   "source": [
    "As each brain layer category is a string, we should use their values to get a tensor out of them instead. This can be easily done in AnnData:\n",
    "\n",
    "- `Layer 1` -> `0`\n",
    "- `Layer 2` -> `1`\n",
    "- `Layer 3` -> `2`\n",
    "- `Layer 4` -> `3`\n",
    "- `Layer 5` -> `4`\n",
    "- `Layer 6` -> `5`\n",
    "- `Layer WM` -> `6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6919fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer categories: AAACAAGTATCTCCCA-1    Layer3\n",
      "AAACAATCTACTAGCA-1    Layer1\n",
      "AAACACCAATAACTGC-1        WM\n",
      "AAACAGAGCGACTCCT-1    Layer3\n",
      "Name: sce.layer_guess, dtype: category\n",
      "Categories (7, object): ['Layer1', 'Layer2', 'Layer3', 'Layer4', 'Layer5', 'Layer6', 'WM']\n",
      "--------\n",
      "layer codes: AAACAAGTATCTCCCA-1    2\n",
      "AAACAATCTACTAGCA-1    0\n",
      "AAACACCAATAACTGC-1    6\n",
      "AAACAGAGCGACTCCT-1    2\n",
      "dtype: int8\n"
     ]
    }
   ],
   "source": [
    "print(\"layer categories:\" , ann_data['151676'].obs[\"sce.layer_guess\"].head(4))\n",
    "print(\"--------\")\n",
    "print(\"layer codes:\" , ann_data['151676'].obs[\"sce.layer_guess\"].cat.codes.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdd280",
   "metadata": {},
   "source": [
    "There are a few NaN s in the dataset. Convert them to a new layer for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7af21b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id: 151676\n",
      "patient_id: 151669\n",
      "patient_id: 151507\n",
      "patient_id: 151508\n",
      "patient_id: 151672\n",
      "patient_id: 151670\n",
      "patient_id: 151673\n",
      "patient_id: 151675\n",
      "patient_id: 151510\n",
      "patient_id: 151671\n",
      "patient_id: 151674\n",
      "patient_id: 151509\n",
      "tensor([2, 0, 6,  ..., 6, 5, 0])\n",
      "tensor([1, 3, 0,  ..., 3, 2, 0])\n",
      "tensor([0, 2, 0,  ..., 6, 5, 0])\n",
      "tensor([2, 0, 6,  ..., 4, 6, 0])\n",
      "tensor([2, 3, 0,  ..., 3, 3, 0])\n",
      "tensor([1, 3, 0,  ..., 3, 2, 0])\n",
      "tensor([2, 0, 6,  ..., 5, 6, 1])\n",
      "tensor([0, 2, 6,  ..., 6, 5, 6])\n",
      "tensor([0, 4, 2,  ..., 2, 5, 2])\n",
      "tensor([2, 3, 0,  ..., 2, 4, 0])\n",
      "tensor([2, 0, 6,  ..., 5, 6, 0])\n",
      "tensor([0, 2, 5,  ..., 3, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_data_y\n",
    "\n",
    "data_y = get_data_y(ann_data)\n",
    "for patient_id in data_y.keys():\n",
    "    print(data_y[patient_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544b7fe",
   "metadata": {},
   "source": [
    "## data.pos\n",
    "Node position matrix with shape [num_nodes, num_dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bdce801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3460, 2])\n",
      "torch.Size([3661, 2])\n",
      "torch.Size([4226, 2])\n",
      "torch.Size([4384, 2])\n",
      "torch.Size([4015, 2])\n",
      "torch.Size([3498, 2])\n",
      "torch.Size([3639, 2])\n",
      "torch.Size([3592, 2])\n",
      "torch.Size([4634, 2])\n",
      "torch.Size([4110, 2])\n",
      "torch.Size([3673, 2])\n",
      "torch.Size([4789, 2])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_data_pos\n",
    "\n",
    "data_pos = get_data_pos(ann_data)\n",
    "for patient_id in data_pos.keys():\n",
    "    print(data_pos[patient_id].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95772931",
   "metadata": {},
   "source": [
    "## Creating the Data Loaders with all the gathered information\n",
    "One for each group:\n",
    "- Training\n",
    "- Validation\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0669721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from dataloader import get_dataloaders\n",
    "\n",
    "params = yaml.safe_load(open(\"params.yaml\"))['train']\n",
    "\n",
    "patients = (train_patients, val_patients, test_patients)\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(patients, data_x, \\\n",
    "                                                        edge_indices, edge_features, \\\n",
    "                                                        data_pos, data_y, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af9912cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31731\n",
      "0        Layer I      8631     27.20%\n",
      "1        Layer II     2541     8.01%\n",
      "2        Layer III    7977     25.14%\n",
      "3        Layer IV     3494     11.01%\n",
      "4        Layer V      4121     12.99%\n",
      "5        Layer VI     2831     8.92%\n",
      "6        White Matter 2037     6.42%\n",
      "7        Unknown      99       0.31%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 8\n",
    "layer_names = ['Layer I', 'Layer II', 'Layer III', 'Layer IV', 'Layer V', 'Layer VI', 'White Matter', 'Unknown']\n",
    "\n",
    "def class_ocurrences(loader, num_classes):\n",
    "    class_counts = torch.zeros(num_classes, dtype=torch.long)\n",
    "\n",
    "    for batch in loader:\n",
    "        class_counts += torch.bincount(batch.y, minlength=num_classes)\n",
    "\n",
    "    total_samples = class_counts.sum().item()\n",
    "    print(total_samples)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        count = class_counts[i].item()\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"{i:<8} {layer_names[i]:<12} {count:<8} {percentage:.2f}%\")\n",
    "\n",
    "        \n",
    "class_ocurrences(train_loader, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed9087",
   "metadata": {},
   "source": [
    "### Optional: Visualize one graph\n",
    "\n",
    "Takes a lot of time to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beecb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader import visualize_data\n",
    "\n",
    "\n",
    "# visualize_data(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a3df4",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f867441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import get_model\n",
    "\n",
    "with open('params.yaml') as parfile:\n",
    "    all_params = yaml.safe_load(parfile)\n",
    "    featurize_params = all_params['featurize']\n",
    "    train_params = all_params['train']\n",
    "    train_params['pca_components'] = featurize_params['pca_components']\n",
    "    train_params['seed'] = featurize_params['seed']\n",
    "    tracking_params = all_params['tracking']\n",
    "\n",
    "device, model = get_model(train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597e0f3",
   "metadata": {},
   "source": [
    "## Optimizer, Loss and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07f61776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import get_criterion, get_optimizer, get_scheduler\n",
    "\n",
    "# obtain class frequencies\n",
    "class_counts = torch.zeros(model.num_classes, dtype=torch.long)\n",
    "for batch in train_loader:\n",
    "    class_counts += torch.bincount(batch.y, minlength=model.num_classes)\n",
    "total = class_counts.sum().item()\n",
    "class_freqs = class_counts.float() / total\n",
    "class_freqs = class_freqs.to(device)\n",
    "\n",
    "optimizer = get_optimizer(model, train_params)\n",
    "criterion = get_criterion(class_freqs, train_params)\n",
    "scheduler = get_scheduler(optimizer, train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d5864",
   "metadata": {},
   "source": [
    "## Setting experiment tracking\n",
    "\n",
    "We use [mlflow](https://mlflow.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43fb71e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow server started (PID 230904)\n"
     ]
    }
   ],
   "source": [
    "from mlflow_server import start_mlflow_server, stop_mlflow_server\n",
    "\n",
    "port=\"5000\"\n",
    "artifacts_dir = \"artifacts\"\n",
    "pid_file_path = \"mlflow.pid\"\n",
    "log_dir=\"logs\"\n",
    "experiment_name = \"BrainLayerClassifier\"\n",
    "\n",
    "start_mlflow_server(port=port, artifacts_dir=artifacts_dir, pid_file_path=pid_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203914d3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8463453c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m writer = start_tracking_experiment(exp_name=experiment_name, port=port, log_dir=log_dir)\n\u001b[32m      6\u001b[39m loaders = (train_loader, val_loader)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/GNNCellClassification/src/train.py:92\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, optimizer, criterion, scheduler, loaders, device, params, writer)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_loop\u001b[39m(model, optimizer, criterion, scheduler, loaders, device, params, writer):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     train_loader, val_loader, test_loader = loaders\n\u001b[32m     93\u001b[39m     num_epochs = params[\u001b[33m\"\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     95\u001b[39m     early_stopping_params = params[\u001b[33m'\u001b[39m\u001b[33mearly_stopping\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from train import train_loop, start_tracking_experiment\n",
    "\n",
    "\n",
    "writer = start_tracking_experiment(exp_name=experiment_name, port=port, log_dir=log_dir)\n",
    "\n",
    "loaders = (train_loader, val_loader)\n",
    "train_loop(model, optimizer, criterion, scheduler, loaders, device, params, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212be173",
   "metadata": {},
   "source": [
    "## Stopping experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ab5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_server import stop_mlflow_server\n",
    "\n",
    "stop_mlflow_server(pid_file_path=pid_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (general-venv)",
   "language": "python",
   "name": "general-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
